{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Demo\n",
    "\n",
    "If you haven't installed spacy yet, use:\n",
    "```\n",
    "conda install spacy\n",
    "python -m spacy.en.download\n",
    "```\n",
    "This downloads about 500 MB of data.\n",
    "\n",
    "Another popular package, `nltk`, can be installed as follows (you can skip this for now):\n",
    "\n",
    "```\n",
    "conda install nltk\n",
    "python -m nltk.downloader all\n",
    "```\n",
    "\n",
    "This also downloads a lot of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load StumbleUpon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"stumbleupon.tsv\", sep='\\t',\n",
    "                  encoding=\"utf-8\")\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load spacy\n",
    "import spacy\n",
    "nlp_toolkit = spacy.load(\"en\")\n",
    "\n",
    "\n",
    "from spacy.en import English\n",
    "nlp_toolkit = English()\n",
    "nlp_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to load `spacy`:\n",
    "```\n",
    "import spacy\n",
    "nlp_toolkit = spacy.load(\"en\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you got an error above:\n",
    "\n",
    "+ Model 'en>=1.1.0,<1.2.0' not installed. Please run 'python -m spacy.en.download' to install latest compatible model.\n",
    "\n",
    "Try running this in terminal:\n",
    "\n",
    "\n",
    "    conda create -n spacy python\n",
    "\n",
    "    source activate spacy\n",
    "\n",
    "    conda install spacy\n",
    "\n",
    "    python -m spacy.en.download\n",
    "    \n",
    "    python -c \"import spacy; spacy.load('en')\"\n",
    "    \n",
    "+ note: this didn't work for me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = u\"IBM sees holographic calls, air breathing batteries\"\n",
    "parsed = nlp_toolkit(title)\n",
    "\n",
    "for (i, word) in enumerate(parsed): \n",
    "    print \"Word: {}\".format(word)\n",
    "    print \"\\t Phrase type: {}\".format(word.dep_)\n",
    "    print \"\\t Is the word a known entity type? {}\".format(\n",
    "        word.ent_type_  if word.ent_type_ else \"No\")\n",
    "    print \"\\t Lemma: {}\".format(word.lemma_)\n",
    "    print \"\\t Parent of this word: {}\".format(word.head.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Page Titles\n",
    "\n",
    "Let's see if we can find organizations in our page titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def references_organization(title):\n",
    "    parsed = nlp_toolkit(title)\n",
    "    return any([word.ent_type_ == 'ORG' for word in parsed])\n",
    "\n",
    "data['references_organization'] = data['title'].fillna(u'').map(references_organization)\n",
    "\n",
    "# Take a look\n",
    "data[data['references_organization']][['title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Exercise solution\n",
    "\n",
    "def references_org_person(title):\n",
    "    parsed = nlp_toolkit(title)\n",
    "    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n",
    "    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n",
    "    return contains_org and contains_person\n",
    "\n",
    "data['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\n",
    "\n",
    "# Take a look\n",
    "data[data['references_org_person']][['title']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting \"Greenness\" Of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset comes from [stumbleupon](https://www.stumbleupon.com/), a web page recommender.  \n",
    "\n",
    "A description of the columns is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FieldName|Type|Description\n",
    "---------|----|-----------\n",
    "url|string|Url of the webpage to be classified\n",
    "title|string|Title of the article\n",
    "body|string|Body text of article\n",
    "urlid|integer| StumbleUpon's unique identifier for each url\n",
    "boilerplate|json|Boilerplate text\n",
    "alchemy_category|string|Alchemy category (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "alchemy_category_score|double|Alchemy category score (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "avglinksize| double|Average number of words in each link\n",
    "commonlinkratio_1|double|# of links sharing at least 1 word with 1 other links / # of links\n",
    "commonlinkratio_2|double|# of links sharing at least 1 word with 2 other links / # of links\n",
    "commonlinkratio_3|double|# of links sharing at least 1 word with 3 other links / # of links\n",
    "commonlinkratio_4|double|# of links sharing at least 1 word with 4 other links / # of links\n",
    "compression_ratio|double|Compression achieved on this page via gzip (measure of redundancy)\n",
    "embed_ratio|double|Count of number of <embed> usage\n",
    "frameBased|integer (0 or 1)|A page is frame-based (1) if it has no body markup but have a frameset markup\n",
    "frameTagRatio|double|Ratio of iframe markups over total number of markups\n",
    "hasDomainLink|integer (0 or 1)|True (1) if it contains an <a> with an url with domain\n",
    "html_ratio|double|Ratio of tags vs text in the page\n",
    "image_ratio|double|Ratio of <img> tags vs text in the page\n",
    "is_news|integer (0 or 1) | True (1) if StumbleUpon's news classifier determines that this webpage is news\n",
    "lengthyLinkDomain| integer (0 or 1)|True (1) if at least 3 <a> 's text contains more than 30 alphanumeric characters\n",
    "linkwordscore|double|Percentage of words on the page that are in hyperlink's text\n",
    "news_front_page| integer (0 or 1)|True (1) if StumbleUpon's news classifier determines that this webpage is front-page news\n",
    "non_markup_alphanum_characters|integer| Page's text's number of alphanumeric characters\n",
    "numberOfLinks|integer Number of <a>|markups\n",
    "numwords_in_url| double|Number of words in url\n",
    "parametrizedLinkRatio|double|A link is parametrized if it's url contains parameters or has an attached onClick event\n",
    "spelling_errors_ratio|double|Ratio of words not found in wiki (considered to be a spelling mistake)\n",
    "label|integer (0 or 1)|User-determined label. Either evergreen (1) or non-evergreen (0); available for train.tsv only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Let's try extracting some of the text content.\n",
    "> ### Create a feature for the title containing 'recipe'. Is the % of evegreen websites higher or lower on pages that have recipe in the the title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Option 1: Create a function to check for this\n",
    "\n",
    "def has_recipe(text_in):\n",
    "    try:\n",
    "        if 'recipe' in str(text_in).lower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except: \n",
    "        return 0\n",
    "        \n",
    "data['recipe'] = data['title'].map(has_recipe)\n",
    "\n",
    "# Option 2: lambda functions\n",
    "\n",
    "#data['recipe'] = data['title'].map(lambda t: 1 if 'recipe' in str(t).lower() else 0)\n",
    "\n",
    "\n",
    "# Option 3: string functions\n",
    "data['recipe'] = data['title'].str.contains('recipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO:  compare count vectorizer and tfidfvectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 1), \n",
    "                             stop_words=None,\n",
    "                             binary=False)\n",
    "\n",
    "tfv = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                             stop_words=None,\n",
    "                             binary=False, norm='l2', use_idf=True)\n",
    "\n",
    "print cv.fit_transform([\"hey I am reid\", \"this is a second sentence\"]).A\n",
    "\n",
    "\n",
    "#print cv.vocabulary_\n",
    "\n",
    "print tfv.fit_transform([\"hey I am reid\", \"this is a second sentence\"]).A\n",
    "\n",
    "\n",
    "#tfv.vocabulary_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Use of the Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = data['title'].fillna('')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, \n",
    "                             ngram_range=(1, 2), \n",
    "                             stop_words='english',\n",
    "                             binary=True)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Build a random forest model to predict evergreeness of a website using the title features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 20)\n",
    "    \n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles).toarray()\n",
    "y = data['label']\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build a random forest model to predict evergreeness of a website using both title features and more \"quantitative\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Build a random forest model to predict evergreeness of a website using the body features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Use `TfIdfVectorizer` instead of `CountVectorizer` - is this an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
