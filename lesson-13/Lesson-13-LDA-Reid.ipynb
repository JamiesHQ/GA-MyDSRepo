{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "+ Most commonly used in natural language processing\n",
    "+ Sometimes as an end in and of itself\n",
    "+ Sometimes as a variable reduction technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example of LDA in NLP\n",
    "\n",
    "Stolen from: http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "+ Authors: \n",
    "    + Olivier Grisel <olivier.grisel@ensta.org>\n",
    "    + Lars Buitinck\n",
    "    + Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "+ License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code defines a custom function that we'll use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code loads the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.913s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to continue this think tank charade of yours, your\\nfixation on Israel must stop.  You might have to start asking the\\nsame sort of questions of Arab countries as well.  You realize it\\nwould not work, as the Arab countries' treatment of Jews over the\\nlast several decades is so bad that your fixation on Israel would\\nbegin to look like the biased attack that it is.\\n\\n   Everyone in this group recognizes that your stupid 'Center for\\nPolicy Research' is nothing more than a fancy name for some bigot\\nwho hates Israel.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['data'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.402s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 12.535s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_example = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.530256</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.240672</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.204932</td>\n",
       "      <td>0.003449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.551005</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.423990</td>\n",
       "      <td>0.003125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.296628</td>\n",
       "      <td>0.284906</td>\n",
       "      <td>0.003573</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.393462</td>\n",
       "      <td>0.003572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.244747</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.429914</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.080891</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.227779</td>\n",
       "      <td>0.002778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.126397</td>\n",
       "      <td>0.307343</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.529411</td>\n",
       "      <td>0.005264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.506527</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.420736</td>\n",
       "      <td>0.009093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.086142</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.042762</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.299677</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.371743</td>\n",
       "      <td>0.186517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.520215</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.404932</td>\n",
       "      <td>0.050709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.325559</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.636338</td>\n",
       "      <td>0.004762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.449908</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.152455</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.380560</td>\n",
       "      <td>0.002440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.006672</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.044404</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.856673</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.063408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.007145</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>0.185779</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.346036</td>\n",
       "      <td>0.418177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.259376</td>\n",
       "      <td>0.020008</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.245759</td>\n",
       "      <td>0.020005</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.354845</td>\n",
       "      <td>0.020001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.088127</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.827193</td>\n",
       "      <td>0.052856</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.004546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.014287</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.144153</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.014287</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>0.592016</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014294</td>\n",
       "      <td>0.163806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.011115</td>\n",
       "      <td>0.534143</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.011115</td>\n",
       "      <td>0.376952</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011114</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.186988</td>\n",
       "      <td>0.350616</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.439059</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.003334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.012501</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.176864</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.723121</td>\n",
       "      <td>0.012503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.011112</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.011114</td>\n",
       "      <td>0.899982</td>\n",
       "      <td>0.011114</td>\n",
       "      <td>0.011115</td>\n",
       "      <td>0.011114</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011115</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.988887</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.335888</td>\n",
       "      <td>0.084849</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.510607</td>\n",
       "      <td>0.046429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.521791</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.453964</td>\n",
       "      <td>0.003030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.414445</td>\n",
       "      <td>0.189797</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.365318</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.004348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.629350</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.342074</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.003571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.214656</td>\n",
       "      <td>0.082634</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.054556</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.506883</td>\n",
       "      <td>0.130631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.046090</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.866390</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.077659</td>\n",
       "      <td>0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.440669</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.216513</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.272805</td>\n",
       "      <td>0.010001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.988460</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.079311</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.438630</td>\n",
       "      <td>0.475637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>0.190197</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.743126</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>0.033336</td>\n",
       "      <td>0.033343</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.033349</td>\n",
       "      <td>0.699899</td>\n",
       "      <td>0.033347</td>\n",
       "      <td>0.033353</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033368</td>\n",
       "      <td>0.033334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11286</th>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.261859</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.076623</td>\n",
       "      <td>0.644442</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.002439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11287</th>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.891538</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.086837</td>\n",
       "      <td>0.002703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11288</th>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.208161</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.489058</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016670</td>\n",
       "      <td>0.186104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>0.430897</td>\n",
       "      <td>0.165158</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.367096</td>\n",
       "      <td>0.005264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>0.005557</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.949996</td>\n",
       "      <td>0.005556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.054608</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.372396</td>\n",
       "      <td>0.189581</td>\n",
       "      <td>0.372503</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.001819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11292</th>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.976311</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11293</th>\n",
       "      <td>0.020004</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.819989</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.020002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11294</th>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.191282</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.299916</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.462130</td>\n",
       "      <td>0.006667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11295</th>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.266066</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.701928</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11296</th>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.337127</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.350739</td>\n",
       "      <td>0.179027</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.113749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11297</th>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.219021</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.432192</td>\n",
       "      <td>0.278778</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11298</th>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.096442</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.819409</td>\n",
       "      <td>0.004349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11299</th>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.010005</td>\n",
       "      <td>0.147053</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.772935</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11300</th>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.056127</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.704608</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.234154</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11301</th>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.142566</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.694866</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.143646</td>\n",
       "      <td>0.002703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.801106</td>\n",
       "      <td>0.168121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11303</th>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.909986</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11304</th>\n",
       "      <td>0.055510</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.926305</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11305</th>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.166707</td>\n",
       "      <td>0.099360</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.717261</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11306</th>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.334964</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.059257</td>\n",
       "      <td>0.585188</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11307</th>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.011948</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.982457</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11308</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.103881</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.816107</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>0.887485</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012501</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012501</td>\n",
       "      <td>0.012505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014287</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.014287</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.871418</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>0.011115</td>\n",
       "      <td>0.011117</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.011114</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.011115</td>\n",
       "      <td>0.899974</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011116</td>\n",
       "      <td>0.011112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.030619</td>\n",
       "      <td>0.098501</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.049135</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.811396</td>\n",
       "      <td>0.001724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.530256  0.003449  0.003449  0.003449  0.003449  0.240672  0.003449   \n",
       "1      0.003125  0.003125  0.003125  0.003126  0.003127  0.551005  0.003126   \n",
       "2      0.296628  0.284906  0.003573  0.003572  0.003572  0.003572  0.003572   \n",
       "3      0.002778  0.244747  0.002779  0.429914  0.002778  0.002779  0.080891   \n",
       "4      0.005264  0.005265  0.005266  0.005264  0.126397  0.307343  0.005264   \n",
       "5      0.009092  0.009092  0.009092  0.009093  0.009092  0.506527  0.009092   \n",
       "6      0.002632  0.086142  0.002632  0.002632  0.042762  0.002632  0.299677   \n",
       "7      0.003450  0.003449  0.003450  0.003449  0.003449  0.520215  0.003449   \n",
       "8      0.004762  0.325559  0.004764  0.004763  0.004764  0.004763  0.004763   \n",
       "9      0.002439  0.002440  0.002439  0.449908  0.002439  0.002440  0.152455   \n",
       "10     0.006672  0.000038  0.044404  0.002639  0.856673  0.000038  0.016589   \n",
       "11     0.007145  0.007144  0.007144  0.007144  0.007143  0.007144  0.185779   \n",
       "12     0.020002  0.259376  0.020008  0.020003  0.245759  0.020005  0.020002   \n",
       "13     0.004548  0.088127  0.004547  0.004546  0.004546  0.827193  0.052856   \n",
       "14     0.014287  0.014290  0.144153  0.014288  0.014287  0.014293  0.592016   \n",
       "15     0.011115  0.534143  0.011113  0.011115  0.376952  0.011113  0.011112   \n",
       "16     0.186988  0.350616  0.003334  0.439059  0.003334  0.003334  0.003334   \n",
       "17     0.012501  0.012502  0.176864  0.012502  0.012502  0.012502  0.012502   \n",
       "18     0.011112  0.011113  0.011114  0.899982  0.011114  0.011115  0.011114   \n",
       "19     0.001235  0.001235  0.001235  0.001235  0.001235  0.988887  0.001235   \n",
       "20     0.003704  0.003704  0.003705  0.335888  0.084849  0.003704  0.003705   \n",
       "21     0.003031  0.003031  0.003031  0.521791  0.003031  0.003031  0.003031   \n",
       "22     0.004349  0.414445  0.189797  0.004349  0.004349  0.365318  0.004348   \n",
       "23     0.003572  0.629350  0.003572  0.003572  0.003572  0.342074  0.003572   \n",
       "24     0.214656  0.082634  0.002128  0.002128  0.054556  0.002128  0.002128   \n",
       "25     0.001409  0.001409  0.001409  0.046090  0.001409  0.866390  0.001409   \n",
       "26     0.010003  0.440669  0.010001  0.010003  0.216513  0.010004  0.010002   \n",
       "27     0.001282  0.001283  0.001282  0.988460  0.001282  0.001283  0.001282   \n",
       "28     0.079311  0.000917  0.000917  0.000917  0.000917  0.000917  0.000917   \n",
       "29     0.008334  0.008336  0.190197  0.008335  0.008334  0.008336  0.008335   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "11284  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "11285  0.033336  0.033343  0.033337  0.033349  0.699899  0.033347  0.033353   \n",
       "11286  0.002440  0.261859  0.002440  0.076623  0.644442  0.002440  0.002440   \n",
       "11287  0.002703  0.002703  0.002703  0.002703  0.002703  0.891538  0.002704   \n",
       "11288  0.016668  0.016668  0.016667  0.208161  0.016669  0.016668  0.489058   \n",
       "11289  0.430897  0.165158  0.005265  0.005264  0.005264  0.005265  0.005264   \n",
       "11290  0.005557  0.005556  0.005556  0.005556  0.005556  0.005556  0.005556   \n",
       "11291  0.001818  0.001819  0.054608  0.001819  0.372396  0.189581  0.372503   \n",
       "11292  0.002632  0.002632  0.002632  0.002632  0.002633  0.002632  0.976311   \n",
       "11293  0.020004  0.020001  0.020002  0.020000  0.819989  0.020001  0.020001   \n",
       "11294  0.006667  0.006668  0.006668  0.191282  0.006667  0.006668  0.299916   \n",
       "11295  0.004000  0.004001  0.004000  0.266066  0.004000  0.004001  0.004001   \n",
       "11296  0.003227  0.003227  0.003227  0.337127  0.003227  0.350739  0.179027   \n",
       "11297  0.010001  0.010002  0.219021  0.010003  0.432192  0.278778  0.010002   \n",
       "11298  0.004349  0.004349  0.004348  0.004348  0.053711  0.096442  0.004348   \n",
       "11299  0.010001  0.010003  0.010001  0.010005  0.147053  0.010001  0.772935   \n",
       "11300  0.000730  0.000730  0.056127  0.000730  0.000730  0.704608  0.000730   \n",
       "11301  0.002703  0.002703  0.002703  0.142566  0.002703  0.002704  0.694866   \n",
       "11302  0.003847  0.003847  0.003847  0.003847  0.003846  0.003847  0.003847   \n",
       "11303  0.010001  0.010002  0.010003  0.010002  0.010001  0.909986  0.010002   \n",
       "11304  0.055510  0.002273  0.002273  0.002273  0.002274  0.926305  0.002273   \n",
       "11305  0.002382  0.002382  0.002381  0.166707  0.099360  0.002382  0.717261   \n",
       "11306  0.002941  0.002942  0.334964  0.002942  0.002942  0.059257  0.585188   \n",
       "11307  0.000699  0.011948  0.000699  0.000699  0.000699  0.982457  0.000699   \n",
       "11308  0.010000  0.103881  0.010003  0.010002  0.010002  0.010003  0.010001   \n",
       "11309  0.887485  0.012502  0.012501  0.012502  0.012502  0.012502  0.012500   \n",
       "11310  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "11311  0.014286  0.014287  0.014286  0.014288  0.014286  0.014290  0.014287   \n",
       "11312  0.011115  0.011117  0.011113  0.011114  0.011113  0.011115  0.899974   \n",
       "11313  0.001724  0.001725  0.030619  0.098501  0.001725  0.049135  0.001725   \n",
       "\n",
       "              7         8         9  \n",
       "0      0.003448  0.204932  0.003449  \n",
       "1      0.003125  0.423990  0.003125  \n",
       "2      0.003571  0.393462  0.003572  \n",
       "3      0.002778  0.227779  0.002778  \n",
       "4      0.005263  0.529411  0.005264  \n",
       "5      0.009091  0.420736  0.009093  \n",
       "6      0.002632  0.371743  0.186517  \n",
       "7      0.003448  0.404932  0.050709  \n",
       "8      0.004762  0.636338  0.004762  \n",
       "9      0.002439  0.380560  0.002440  \n",
       "10     0.000038  0.009500  0.063408  \n",
       "11     0.007143  0.346036  0.418177  \n",
       "12     0.020000  0.354845  0.020001  \n",
       "13     0.004546  0.004547  0.004546  \n",
       "14     0.014286  0.014294  0.163806  \n",
       "15     0.011111  0.011114  0.011111  \n",
       "16     0.003333  0.003334  0.003334  \n",
       "17     0.012500  0.723121  0.012503  \n",
       "18     0.011111  0.011115  0.011111  \n",
       "19     0.001235  0.001235  0.001235  \n",
       "20     0.003704  0.510607  0.046429  \n",
       "21     0.003030  0.453964  0.003030  \n",
       "22     0.004348  0.004349  0.004348  \n",
       "23     0.003571  0.003572  0.003571  \n",
       "24     0.002128  0.506883  0.130631  \n",
       "25     0.001408  0.077659  0.001409  \n",
       "26     0.010000  0.272805  0.010001  \n",
       "27     0.001282  0.001282  0.001282  \n",
       "28     0.000917  0.438630  0.475637  \n",
       "29     0.008333  0.743126  0.008333  \n",
       "...         ...       ...       ...  \n",
       "11284  0.100000  0.100000  0.100000  \n",
       "11285  0.033333  0.033368  0.033334  \n",
       "11286  0.002439  0.002440  0.002439  \n",
       "11287  0.002703  0.086837  0.002703  \n",
       "11288  0.016667  0.016670  0.186104  \n",
       "11289  0.005263  0.367096  0.005264  \n",
       "11290  0.005556  0.949996  0.005556  \n",
       "11291  0.001818  0.001819  0.001819  \n",
       "11292  0.002632  0.002632  0.002632  \n",
       "11293  0.020000  0.020001  0.020002  \n",
       "11294  0.006667  0.462130  0.006667  \n",
       "11295  0.004001  0.701928  0.004001  \n",
       "11296  0.003226  0.003227  0.113749  \n",
       "11297  0.010000  0.010003  0.010000  \n",
       "11298  0.004348  0.819409  0.004349  \n",
       "11299  0.010000  0.010002  0.010000  \n",
       "11300  0.000730  0.234154  0.000730  \n",
       "11301  0.002703  0.143646  0.002703  \n",
       "11302  0.003846  0.801106  0.168121  \n",
       "11303  0.010000  0.010003  0.010000  \n",
       "11304  0.002273  0.002273  0.002273  \n",
       "11305  0.002381  0.002382  0.002382  \n",
       "11306  0.002941  0.002942  0.002941  \n",
       "11307  0.000699  0.000700  0.000699  \n",
       "11308  0.010000  0.816107  0.010000  \n",
       "11309  0.012500  0.012501  0.012505  \n",
       "11310  0.100000  0.100000  0.100000  \n",
       "11311  0.014286  0.871418  0.014286  \n",
       "11312  0.011111  0.011116  0.011112  \n",
       "11313  0.001724  0.811396  0.001724  \n",
       "\n",
       "[11314 rows x 10 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(X_example, columns = [\"ethnic\", \"next topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "people gun armenian armenians war\n",
      "Topic #1:\n",
      "government people law mr use\n",
      "Topic #2:\n",
      "space program output entry data\n",
      "Topic #3:\n",
      "key car chip used keys\n",
      "Topic #4:\n",
      "edu file com available mail\n",
      "Topic #5:\n",
      "god people does jesus say\n",
      "Topic #6:\n",
      "windows use drive thanks does\n",
      "Topic #7:\n",
      "ax max b8f g9v a86\n",
      "Topic #8:\n",
      "just don like think know\n",
      "Topic #9:\n",
      "10 00 25 15 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In class assignment\n",
    "\n",
    "+ load in the training set (done for you below)\n",
    "+ re-run LDA and use topics as input for model\n",
    "+ Predict categories using some multinomial classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.876s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'), \n",
    "                            subset=\"train\")\n",
    "\n",
    "data = dataset.data\n",
    "\n",
    "y = dataset.target\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(y, return_counts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 4.024s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf_vectorizer.fit(data)\n",
    "\n",
    "tf = tf_vectorizer.transform(data)\n",
    "\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 106.853s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=20, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'), \n",
    "                            subset=\"test\")\n",
    "\n",
    "testdata = test.data\n",
    "\n",
    "y_test = test.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier()\n",
    "\n",
    "clf2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "gs = GridSearchCV(estimator = RandomForestClassifier(), \n",
    "                 param_grid = {'n_estimators':np.arange(10, 21, 1)}, \n",
    "                 cv = KFold(n_splits=5))\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "algo = gs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 46,   4,   4,   3,   3,   3,   3,  20,  16,  13,   5,   6,   4,\n",
       "         27,  14,  90,  15,  14,   6,  23],\n",
       "       [  4, 123,  58,  25,  19,  66,   7,  13,   4,   4,   0,  11,  15,\n",
       "          7,  21,   4,   3,   3,   1,   1],\n",
       "       [  1,  65, 101,  47,  31,  48,   6,  27,   9,   3,   3,   9,   8,\n",
       "         14,   7,   3,   2,   3,   7,   0],\n",
       "       [  1,  47,  60, 112,  76,  13,  24,  14,   8,   5,   2,   5,  15,\n",
       "          0,   5,   0,   3,   0,   1,   1],\n",
       "       [  3,  16,  34, 107,  94,  10,  28,  27,  14,   4,   8,   7,  16,\n",
       "          4,   7,   0,   2,   2,   2,   0],\n",
       "       [  4,  90,  72,  19,  11, 121,   6,  19,   6,   5,   2,   8,   6,\n",
       "         11,   5,   2,   0,   3,   3,   2],\n",
       "       [  2,  13,   9,  27,  32,   9, 222,  34,   5,   5,   5,   1,  14,\n",
       "          4,   3,   1,   0,   2,   2,   0],\n",
       "       [ 10,   9,   5,   9,  16,   9,  61, 131,  33,   5,   7,   6,  39,\n",
       "         17,   4,   5,  16,   4,   9,   1],\n",
       "       [ 15,  18,   8,  15,  19,  11,  15,  56, 102,  12,  13,  14,  12,\n",
       "         31,  12,   1,  16,  15,   9,   4],\n",
       "       [ 13,  13,   6,   3,   8,   4,   3,  34,  17, 120, 114,   3,   4,\n",
       "         19,  15,   2,   5,   4,  10,   0],\n",
       "       [  6,   2,   2,   2,   1,   2,   3,  16,  20, 158, 152,   6,   2,\n",
       "         11,   6,   4,   2,   1,   3,   0],\n",
       "       [ 10,   9,   9,  13,  11,   4,  11,  27,  11,   6,   3, 188,  22,\n",
       "         11,   9,   8,  22,  11,  10,   1],\n",
       "       [  5,  33,  29,  37,  33,  26,  23,  42,  21,   3,   2,  36,  40,\n",
       "         20,  23,   3,   4,   2,   6,   5],\n",
       "       [ 26,  20,  16,   4,  11,  12,  11,  40,  28,  10,   9,   7,  18,\n",
       "         85,  37,  13,  13,   9,  20,   7],\n",
       "       [ 14,  25,  13,  13,  11,  14,  10,  40,  26,  10,   3,  14,  20,\n",
       "         57,  65,  12,  19,   7,  17,   4],\n",
       "       [ 52,   7,   5,   5,   4,   4,   5,  21,   6,   1,   4,   3,   4,\n",
       "         10,  12, 207,   6,   7,   7,  28],\n",
       "       [ 36,   6,   1,   4,   5,   6,   4,  29,  20,  17,   5,  22,   9,\n",
       "         26,  17,  16,  81,  28,  26,   6],\n",
       "       [ 21,   2,   1,   1,   3,   6,   1,  16,   5,  11,  10,   9,   3,\n",
       "         18,   8,  25,  26, 185,  17,   8],\n",
       "       [ 25,   3,   4,   5,   1,   2,   3,  17,  11,   5,   4,  15,   8,\n",
       "         31,  16,  18,  56,  30,  48,   8],\n",
       "       [ 35,   5,   6,   2,   5,   4,   1,  12,   7,  11,   6,   3,   3,\n",
       "         13,  10,  72,  11,  18,   8,  19]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28736194294309109"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = tf_vectorizer.transform(testdata)\n",
    "X_test = lda.transform(X_test)\n",
    "\n",
    "pred = clf2.predict(X_test)\n",
    "metrics.f1_score(y_test, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31213489113117365"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In class assignment:\n",
    "\n",
    "+ I'll divide you into 3 segments\n",
    "+ Each segment generates 100 sentences on the *same topic*\n",
    "+ Save as a JSON and send to me\n",
    "+ We'll run them through LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CLASSLIST = [\"My favorite type of food is tacos, but it used to be fried chicken.\", \"My favorite type of taco is al pastor.\", \"My favorite mexican resturant is El Rancho.\", \"I also like all the resturants in my immediate neighborhood.\", \"Corn dogs are quite nice as well however my friends make fun of me\", \"Yo dog I love dog, but not like the food\", \"Sup hot stuff, you like hot food or cold food\", \"I raise chickens in my farm that I dont eat\", \"Sometimes I go fishing with my father\", \"Working at the food bank is very fulfilling to me\", \"If I eat too much Im not going to feel like drinking\", \"The breweries in San Diego are plentiful\", \"The food in San Deigo are not as good as the stuff in SF\", \"LA has really solid mexican food which I love\", \"Im pretty hungry right now, where should we grab lunch?\", \"Is dinner going to be taken care of at the Reynold's house?\", \"If I pay for breakfast will you cover lunch or dinner babe?\", \"Fast Food is not good for health\", \"Indian food is spicy\", \"I like Thai food\", \"There are two new restaurants opened around the block\", \"Can I get this sweet dish?\", \"McBurger has 3000 calories\", \"Nuts are good for health\", \"Vegetables are bad\", \"Cheese cake is good\", \"He ate all the fried food\", \"In istanbul, a burger cost $30\", \"The new hotel chain offers free buffet for 2 days\", \"Can I get a diet coke?\", \"How to cook fiesta salsa?\", \"These fries are tasty but bad for health\", \"This chinese restaurant serves the best soup\", \"Please order a pizza for me?\", \"Dinner is ready\", \"Doing breakfast is good for health\", \"Please dont throw extra food, donate it to someone hungry\", \"chocolate chip cookies and best fresh from the oven.\", \"pumpkin pie is a good dessert for the fall season\", \"vegtables are an important part of any diet\", \"fruit is a healthy way to suffice your sweet tooth\", \"eggs are a filling way eat breakfast\", \"soda is a necessary evil.\", \"philz coffee is a great way to start your morning\", \"after making a big dinner with several courses, at least there are leftovers.\", \"turnkey is a great type of meat\", \"hot sauce makes everything better.\", \"hot dogs and garlic fries are best when watching a giants baseball game.\", \"I like ketchup more than mustard\", \"I wish a had a few more cook books.\", \"The worst part of cooking is cleaning the pots and pans afterwards.\", \"I had cereal with a banana every morning before school as a kid.\", \"Avocado is my favorite type of vegtable.\", \"I try to avoid fast food restaurants as much as possible.\", \"shrimp scampi is one of my all time favorite dishes.\", \"cooking is something I hope to do more of later in life.\", \"salmon is a great type of food\", \"You should eat well, but not like Charles Barkley well.\", \"There are like 17 cooking shows. All of them seem to be related to Top Chef.\", \"Guy Fearri is not a chef so much as the lead from Smashmouth pretending to be a chef.\", \"Salt is not a food. But it goes well on food.\", \"Vegetarians who still eat fish are not vegetarians. They are just against eating things that have eyes.\", \"Vegans are basically food Taliban. Do not make me feel bad because I have good things in my life.\", \"They say cows shitting causes global warming. That means we should eat less cows. Maybe more veal though. What is the shit to meat produced ratio where we can still enjoy meat, but not destroy the only planet we have.\", \"My mother said pre-heat the oven. Instead I turned on the microwave.\", \"Turkey is the worst of the bird dishes.\", \"Dog is a food someplaces.\", \"To make rice, you just get rice, and then add water.\", \"Food Trucks are not made of food.\", \"Instagram is mostly a forum for posting food photos. ALso for Smirnoff ICe ads.\", \"Pasta is a delicacy.\", \"I refused to believe that gushers are a food.\", \"If you travel exclusively for local dishes, you have too much money.\", \"Happiness: a good bank account, a good cook, and a good digestion.\", \"Food Porn and Porn Food are not the same thing, and you should google only one.\", \"France thinks it has the best food in Europe, but really Italy does. In Asia, Thailand is to France, as Vietnam is to Italy. I will not negotiate on this.\"]\n",
    "\n",
    "sports = {\"Food\": CLASSLIST}\n",
    "\n",
    "import json\n",
    "\n",
    "with open('Food.json', 'w') as fp:\n",
    "    json.dump(sports, fp, sort_keys=True, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=10, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "travel = {\"Travel\":travellist}\n",
    "\n",
    "### JSON save:\n",
    "\n",
    "import json\n",
    "with open('Travel.json', 'w') as fp:\n",
    "    json.dump(travel, fp, sort_keys=True, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is where we import our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#This code imports a json of ALL viable glooko codes - before random selection\n",
    "\n",
    "import json\n",
    "with open('Food.json', 'r') as fp:\n",
    "    food = json.load(fp)\n",
    "\n",
    "with open('sports.json', 'r') as fp:\n",
    "    sports = json.load(fp)\n",
    "    \n",
    "with open('travel.json', 'r') as fp:\n",
    "    travel = json.load(fp)\n",
    "\n",
    "\n",
    "sentencelist = []\n",
    "\n",
    "sentencelist.extend(food['Food'])\n",
    "sentencelist.extend(sports['Sports'])\n",
    "sentencelist.extend(travel['Travel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm using this code to create an outcome variable, \n",
    "\n",
    "+ so we can test our topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "y.extend([0]*len(food['Food']))\n",
    "y.extend([1]*len(sports['Sports']))\n",
    "y.extend([2]*len(travel['Travel']))\n",
    "\n",
    "label = []\n",
    "label.extend([\"Food\"]*len(food['Food']))\n",
    "label.extend([\"Sports\"]*len(sports['Sports']))\n",
    "label.extend([\"Travel\"]*len(travel['Travel']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This organizes everything into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"y\":y, \"sentence\":sentencelist, \"label\":label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This splits our sentences and outcome var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing! \n",
    "\n",
    "+ Here is the count vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.011s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=10000,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf_vectorizer.fit(sentencelist)\n",
    "\n",
    "X = tf_vectorizer.transform(sentencelist)\n",
    "\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LDA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=3, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=0.,\n",
    "                                random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=0.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=3, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06681848,  0.08079847,  0.85238305])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docprobs = lda.transform(X)\n",
    "\n",
    "docprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "traveling way best ball tour\n",
      "Topic #1:\n",
      "island time travelers worst ringle\n",
      "Topic #2:\n",
      "food travel rsquo like great\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to split into training and testing!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(docprobs, y, test_size=0.10, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the random forest classifier on X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predy = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 0, 1, 2, 0, 1, 1, 2, 0, 1, 0, 0, 1, 1, 0, 2, 1, 2, 0, 0, 0,\n",
       "       0, 2])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28406432748538007"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, predy, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
