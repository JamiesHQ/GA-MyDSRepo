{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis applied to the Iris dataset.\n",
    "\n",
    "### PART I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN with the original iris\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for c, i, target_name in zip(\"rgb\", [0, 1, 2], target_names):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], c=c, label=target_name)\n",
    "plt.legend()\n",
    "plt.title('IRIS dataset with first two columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### PCA with 2 components \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_r = pca.fit_transform(X)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for c, i, target_name in zip(\"rgb\", [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], c=c, label=target_name)\n",
    "plt.legend()\n",
    "plt.title('PCA(2 components) of IRIS dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the axis are not the same anymore! They are no longer anything such as \"sepal length\" or \"petal width\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_r[:5,]\n",
    "# only 2 columns!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN with PCAed data\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "cross_val_score(knn, X_r, y, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_reconstituted = pca.inverse_transform(X_r)\n",
    "# Turn it back into its 4 column using only 2 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,2], X[:,3])\n",
    "# Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_reconstituted[:,3], X_reconstituted[:,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with 3 components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "X_3 = pca.transform(X)\n",
    "\n",
    "X_3[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for c, i, target_name in zip(\"rgb\", [0, 1, 2], target_names):\n",
    "    plt.scatter(X_3[y == i, 0], X_3[y == i, 1], c=c, label=target_name)\n",
    "plt.legend()\n",
    "plt.title('PCA(3 components) of IRIS dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN with 3 components\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "cross_val_score(knn, X_3, y, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_reconstituted = pca.inverse_transform(X_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,2], X[:,3])\n",
    "# Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_reconstituted[:,2], X_reconstituted[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choosing components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pca = decomposition.PCA(n_components=4)\n",
    "X_r = pca.fit_transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Variance explained by each principal component')\n",
    "plt.ylabel(' % Variance Explained')\n",
    "plt.xlabel('Principal component')\n",
    "\n",
    "# 2 components is enough!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "url = 'yelp.csv'\n",
    "yelp = pd.read_csv(url, encoding='unicode-escape')\n",
    "\n",
    "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART II: PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing no features and using sklearn pipelines\n",
    "\n",
    "# Ask yourself, Why are we using pipelines here?\n",
    "\n",
    "\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "pipe = Pipeline(steps=[('transform', vect), ('lr', lr)])\n",
    "cross_val_score(pipe, X, y, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english', max_features = 1000)\n",
    "pipe = Pipeline(steps=[('transform', vect), ('lr', lr)])\n",
    "cross_val_score(pipe, X, y, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english', max_features = 100)\n",
    "pipe = Pipeline(steps=[('transform', vect), ('lr', lr)])\n",
    "cross_val_score(pipe, X, y, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### make a pipeline that has\n",
    "1. a count vectorizer with english stopwords and 1,000 max features\n",
    "2. a PCA that takes 20 components\n",
    "3. a logistic regression\n",
    "\n",
    "THEN cross validate this pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's wrong with this code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english', max_features=1000)\n",
    "pca = decomposition.PCA(n_components=20, )\n",
    "\n",
    "\n",
    "pipe = Pipeline(steps=[('transform', vect), ('pca', pca), ('lr', lr)])\n",
    "cross_val_score(pipe, X, y, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Didn't work! I'll give you a custom transformer \n",
    "# that just makes any sparse matrix into a dense matrix\n",
    "\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english', max_features=1000)\n",
    "pca = decomposition.PCA(n_components=20)\n",
    "dense = DenseTransformer()\n",
    "\n",
    "pipe = Pipeline(steps=[('transform', vect), ('dense', dense), ('pca', pca), ('lr', lr)])\n",
    "cross_val_score(pipe, X, y, cv=10, scoring='accuracy').mean()\n",
    "\n",
    "# Very comparable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english', max_features=1000)\n",
    "pca = decomposition.PCA(n_components=20)\n",
    "dense = DenseTransformer()\n",
    "\n",
    "pipe = Pipeline(steps=[('transform', vect), ('dense', dense), ('pca', pca), ('lr', lr)])\n",
    "\n",
    "\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_best_worst[yelp_best_worst.stars==5].iloc[100].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe.predict([\"\"\"The food was flavorful and plenty of it. \"\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pipe.predict([\"\"\"The food was awful and disgusting. Service took way too long\"\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "pca = decomposition.PCA(n_components=100)\n",
    "\n",
    "all_dtm = vect.fit_transform(X).todense()\n",
    "pca.fit_transform(all_dtm)\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Variance explained by each principal component')\n",
    "plt.ylabel(' % Variance Explained')\n",
    "plt.xlabel('Principal component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that PCA does require scaled data, but because I knew that each column was on the same unit (\"words\")\n",
    "I did not need to preform any standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BONUS SHAKIRA\n",
    "\n",
    "\n",
    "from pylab import imread, imsave, subplot, imshow, title, gray, figure, show, NullLocator\n",
    "from numpy import mean, cov, dot, linalg, size, argsort\n",
    "\n",
    "def princomp(A, numpc=0):\n",
    "    # computing eigenvalues and eigenvectors of covariance matrix\n",
    "    M = (A-mean(A.T,axis=1)).T # subtract the mean (along columns)\n",
    "    [latent,coeff] = linalg.eig(cov(M))\n",
    "    p = size(coeff,axis=1)\n",
    "    idx = argsort(latent) # sorting the eigenvalues\n",
    "    idx = idx[::-1]       # in ascending order\n",
    "    # sorting eigenvectors according to the sorted eigenvalues\n",
    "    coeff = coeff[:,idx]\n",
    "    latent = latent[idx] # sorting eigenvalues\n",
    "    if numpc < p and numpc >= 0:\n",
    "        coeff = coeff[:,range(numpc)] # cutting some PCs if needed\n",
    "    score = dot(coeff.T,M) # projection of the data in the new space\n",
    "    return coeff,score,latent\n",
    "\n",
    "A = imread('shakira.png') # load an image\n",
    "A = mean(A,2) # to get a 2-D array\n",
    "full_pc = size(A,axis=1) # numbers of all the principal components\n",
    "i = 1\n",
    "dist = []\n",
    "for numpc in range(0,full_pc+10,10): # 0 10 20 ... full_pc\n",
    "    coeff, score, latent = princomp(A,numpc)\n",
    "    Ar = dot(coeff,score).T+mean(A,axis=0) # image reconstruction\n",
    "    # difference in Frobenius norm\n",
    "    dist.append(linalg.norm(A-Ar,'fro'))\n",
    "    # showing the pics reconstructed with less than 50 PCs\n",
    "    print \"trying %s principal components with a distance of: %f\"%(numpc,dist[-1])\n",
    "    if numpc <= 50:\n",
    "        if numpc == 50:\n",
    "            imsave(fname= ''+str(numpc)+'.png', arr = Ar)\n",
    "        ax = subplot(2,3,i,frame_on=False)\n",
    "        ax.xaxis.set_major_locator(NullLocator()) # remove ticks\n",
    "        ax.yaxis.set_major_locator(NullLocator())\n",
    "        i += 1\n",
    "        imshow(Ar)\n",
    "        title('PCs # '+str(numpc))\n",
    "        gray()\n",
    "\n",
    "figure()\n",
    "imshow(A)\n",
    "title('numpc FULL')\n",
    "gray()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BONUS Face recognition:\n",
    "+ (This takes a bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# derived from \n",
    "\n",
    "# http://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "y = lfw_people.target\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[0].reshape((h, w)), cmap=plt.cm.gray)\n",
    "lfw_people.target_names[y[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[100].reshape((h, w)), cmap=plt.cm.gray)\n",
    "lfw_people.target_names[y[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the label to predict is the id of the person\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into a training and testing set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1)\n",
    "# will use x_test as sample OOS data\n",
    "\n",
    "###############################################################################\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 150\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "pca = decomposition.PCA(n_components=n_components, whiten=True).fit(X_train)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "logreg = LogisticRegression()\n",
    "print(\"Fitting the classifier to the training set WITHOUT PCA\")\n",
    "param_grid = {'C': [1e-1,1,1e1]}\n",
    "clf = GridSearchCV(logreg, param_grid)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf = clf.best_estimator_\n",
    "\n",
    "print clf, \"best estimator\"\n",
    "\n",
    "###############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print(\"Predicting people's names on the test set\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print accuracy_score(y_pred, y_test), \"Accuracy\"\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "print (time() - t0), \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "param_grid = {'C': [1e-1,1,1e1]}\n",
    "clf = GridSearchCV(logreg, param_grid)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "clf = clf.best_estimator_\n",
    "\n",
    "print clf, \"best estimator\"\n",
    "\n",
    "t0 = time()\n",
    "###############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print(\"Predicting people's names on the test set WITH PCA\")\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "print accuracy_score(y_pred, y_test), \"Accuracy\"\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "print (time() - t0), \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a pipeline to make this process easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pca = decomposition.PCA(n_components=150, whiten=True)\n",
    "logreg = LogisticRegression(C=1e-1)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = cross_val_score(pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = cross_val_score(logreg, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BUT for ease:\n",
    "t0 = time()\n",
    "logreg.fit(X, y)\n",
    "logreg.predict(X)\n",
    "print (time() - t0), \"Seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "pipe.fit(X, y)\n",
    "pipe.predict(X)\n",
    "print (time() - t0), \"Seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)\n",
    "\n",
    "# plot the gallery of the most significative eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
